{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# things we need for NLP\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# things we need for Tensorflow\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# NER - Getting named entities \n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import nltk\n",
    "import nltk.collocations\n",
    "import nltk.corpus\n",
    "\n",
    "word_list = word_tokenize(open('words.txt').read())\n",
    "punctuations = ['^', ')', '/', '#', '{', '=','-', '~', '|', '`', '&', '$', '_', ',', '\\\\', '?', \"'\", '[', '(', ']', '*', '\"', ':', '}', '%', '<', '.', '>', '!', '@', '+', ';']\n",
    "\n",
    "def processLanguage(text):\n",
    "    new_sent =[]\n",
    "    for t in text:\n",
    "        if t not in punctuations:\n",
    "            new_sent.append(t)\n",
    "        elif t in punctuations:\n",
    "            new_sent.append(' ')\n",
    "            new_sent.append(t)\n",
    "            new_sent.append(' ')\n",
    "    new_sent = ''.join(new_sent)\n",
    "    sentences = sent_tokenize(new_sent)\n",
    "    \n",
    "    try:\n",
    "        proper_nouns =[]\n",
    "        for item in sentences:\n",
    "            l = word_tokenize(item.lower())\n",
    "            s = re.compile(r'\\.')\n",
    "            p = s.sub(r' . ',' '.join(l))\n",
    "            words = word_tokenize(p)\n",
    "            for i in range(len(words)):\n",
    "                if words[i] not in word_list:\n",
    "                    words[i] = words[i].capitalize()\n",
    "                    #print(words[i].capitalize())\n",
    "            item = ' '.join(words)\n",
    "            tokenized = nltk.word_tokenize(item)\n",
    "            tagged = nltk.pos_tag(tokenized)\n",
    "            #print tagged\n",
    "            namedEnt = nltk.ne_chunk(tagged)\n",
    "            #namedEnt.draw()\n",
    "            #print(namedEnt)\n",
    "            for ent in namedEnt:    \n",
    "                if type(ent)==nltk.tree.Tree:\n",
    "                    if type(ent[0]) == tuple:\n",
    "                        #print(ent[0][0])\n",
    "                        proper_nouns.append(ent[0][0])\n",
    "                #print(ent[0])\n",
    "        return [i.lower() for i in proper_nouns]\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import our chat-bot intents file\n",
    "import json\n",
    "with open('new_intents.json') as json_data:\n",
    "    intents = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # add to our words list\n",
    "        words.extend(w)\n",
    "        # add to documents in our corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# remove duplicates\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "    \n",
    "\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "#print(training[:,1])\n",
    "\n",
    "# create train and test lists\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Training phase of intent classifier\n",
    "#Avoid this step if model is already trained and saved .\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    # reset underlying graph data\n",
    "    tf.reset_default_graph()\n",
    "    # Build neural network\n",
    "    table_net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "    table_net = tflearn.fully_connected(table_net, 8)\n",
    "    table_net = tflearn.fully_connected(table_net, 8)\n",
    "    table_net = tflearn.fully_connected(table_net, len(train_y[0]), activation='softmax')\n",
    "    table_net = tflearn.regression(table_net)\n",
    "    # Define model and setup tensorboard\n",
    "    model = tflearn.DNN(table_net, tensorboard_dir='table_tflearn_logs')\n",
    "    # Start training (apply gradient descent algorithm)\n",
    "    model.fit(train_x, train_y, n_epoch=10000, batch_size=8, show_metric=True)\n",
    "    model.save('model.table_tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save all of our data structures\n",
    "import pickle\n",
    "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocessing \n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# convert a sentence into a bag of words model for the neural network to interpret\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model and loading\n",
    "\n",
    "tf.reset_default_graph()\n",
    "# Build neural network\n",
    "table_net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "table_net = tflearn.fully_connected(table_net, 8)\n",
    "table_net = tflearn.fully_connected(table_net, 8)\n",
    "table_net = tflearn.fully_connected(table_net, len(train_y[0]), activation='softmax')\n",
    "table_net = tflearn.regression(table_net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(table_net, tensorboard_dir='table_tflearn_logs')\n",
    "\n",
    "# restore all of our data structures\n",
    "import pickle\n",
    "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
    "words = data['words']\n",
    "classes = data['classes']\n",
    "train_x = data['train_x']\n",
    "train_y = data['train_y']\n",
    "\n",
    "# import our chat-bot intents file\n",
    "import json\n",
    "with open('new_intents.json') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "    \n",
    "# load our saved model\n",
    "model.load('./model.table_tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a data structure to hold user context\n",
    "context = {}\n",
    "\n",
    "ERROR_THRESHOLD = 0.25\n",
    "def classify(sentence):\n",
    "    # generate probabilities from the model\n",
    "    results = model.predict([bow(sentence, words)])[0]\n",
    "    # filter out predictions below a threshold\n",
    "    \n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    #results = [[i,r] for i,r in enumerate(results)]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "    # return tuple of intent and probability\n",
    "    return return_list\n",
    "\n",
    "\n",
    "\n",
    "def response(sentence, userID='123', show_details=False):\n",
    "    results = classify(sentence)\n",
    "    results = [i[0] for i in results]\n",
    "    #results = multi_intent(sentence)\n",
    "    # if we have a classification then find the matching intent tag\n",
    "    if results:\n",
    "        # loop as long as there are matches to process\n",
    "        while results:\n",
    "            for i in intents['intents']:\n",
    "                # find a tag matching the first result\n",
    "                if i['tag'] == results[0]:\n",
    "                    # set context for this intent if necessary\n",
    "                    if 'context_set' in i:\n",
    "                        if show_details: print('context:', i['context_set'])\n",
    "                        context[userID] = i['context_set']\n",
    "\n",
    "                    # check if this intent is contextual and applies to this user's conversation\n",
    "                    if not 'context_filter' in i or \\\n",
    "                        (userID in context and 'context_filter' in i and context[userID] in i['context_filter']):\n",
    "                        if show_details: print ('tag:', i['tag'])\n",
    "                        # a random response from the intent\n",
    "                        return i['function']\n",
    "                    else:\n",
    "                        return \"context_not_set_handler\"\n",
    "            results.pop(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to identify if a single sentence has multiple intents .\n",
    "# Ex : Hello , I want to donate blood . (Output - intents = ['Greeting','want2donate'])\n",
    "\n",
    "def multi_intent(query):\n",
    "    res = [i[0] for i in classify(query)]\n",
    "    #print(res)\n",
    "    if(len(res)>1):\n",
    "        #print(res)\n",
    "        print(\"Multiple intents found\")\n",
    "        return res\n",
    "    else:\n",
    "        sentences = sent_tokenize(query)\n",
    "        #print(sentences)\n",
    "        if(len(sentences)>1):\n",
    "            intents = []\n",
    "            for i in sentences:\n",
    "                #print(classify(i))\n",
    "                sub_intents = classify(i)\n",
    "                for j in sub_intents:\n",
    "                    intents.append(j[0])\n",
    "            #print(intents)\n",
    "            intents = list(set(intents))\n",
    "            if('time' in intents):\n",
    "                intents.remove('time')\n",
    "                intents.append('time')\n",
    "            elif('greeting' in intents):\n",
    "                intents.remove('greeting')\n",
    "                intents = ['greeting']+intents\n",
    "            return intents\n",
    "        else:\n",
    "            words = word_tokenize(query)\n",
    "            sentences = [' '.join(words[:int(len(words)/2)]),' '.join(words[int(len(words)/2):])]\n",
    "            #print(sentences)\n",
    "            intents = []\n",
    "            for i in sentences:\n",
    "                #print(classify(i))\n",
    "                sub_intents = classify(i)\n",
    "                for j in sub_intents:\n",
    "                    if(j[1]>0.5):\n",
    "                        intents.append(j[0])\n",
    "            #print(intents)\n",
    "            intents = list(set(intents))\n",
    "            if('time' in intents):\n",
    "                intents.remove('time')\n",
    "                intents.append('time')\n",
    "            elif('greeting' in intents):\n",
    "                intents.remove('greeting')\n",
    "                intents = ['greeting']+intents\n",
    "            return intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Handlers for each intent\n",
    "\n",
    "\n",
    "import sqlite3 as sql\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "#connecting to database\n",
    "conn=sql.connect(\"blood.db\")\n",
    "\n",
    "def capture_context(user_chat_history):\n",
    "    for i in user_chat_history:\n",
    "        result = classify(i)[0][0]\n",
    "        print(i,end = \"-->\")\n",
    "        print(result)\n",
    "        #print()\n",
    "\n",
    "\n",
    "\n",
    "tables = ['district','bill_payment','blood_recepient','donor','hospital']\n",
    "\n",
    "#handles queries regarding district information\n",
    "def district_handler(query):\n",
    "    result = classify(query)[0][0]\n",
    "    b_grp = ['a+','a-','b+','b-','ab+','ab-','o+','o-']\n",
    "    w = [i.lower() for i in word_tokenize(query)]\n",
    "    blood_group = [i for i in w if i in b_grp]\n",
    "    #print(blood_group)\n",
    "    if(len(blood_group)>0):\n",
    "        query = query.lower()\n",
    "        query = query.split(' ')\n",
    "        for i in query:\n",
    "            if blood_group[0] in i:\n",
    "                query.remove(i)\n",
    "\n",
    "        query = ' '.join(query)\n",
    "        blood_group = blood_group[0]\n",
    "    #print(query)\n",
    "    else:\n",
    "        blood_group = None\n",
    "    details = dict()\n",
    "    details['table'] = \"district\"\n",
    "    details['named_entities'] = processLanguage(query)\n",
    "    if(len(details['named_entities'])==0):\n",
    "        command = \"select dname from district;\"\n",
    "        res = list(conn.execute(command))\n",
    "        l = [\"We operate in \\n\",\"We have our camps in the following districts\\n\",\"You can visit the closest among the following districts\\n\"]\n",
    "        s = random.choice(l)\n",
    "        for i in res:\n",
    "            s = s + i[0]+\"\\n\"\n",
    "        return s\n",
    "    l = []\n",
    "    for i in details['named_entities']:\n",
    "        #print(i)\n",
    "        command = \"select * from \"+ details['table'] +\" where dname= '\"+i+\"';\"\n",
    "        #print(command)\n",
    "        res = list(conn.execute(command))\n",
    "        #print(res)\n",
    "        conn.commit()\n",
    "        if(len(res)>0):\n",
    "            l.append(\"Yes\")\n",
    "        else:\n",
    "            l.append(\"No\")\n",
    "    if(len(l)>0):\n",
    "        return l[0]\n",
    "    else:\n",
    "        return \"Yet to resolve this query\"\n",
    "\n",
    "\n",
    "\n",
    "#Function to get multiple entities from a sentence . \n",
    "#Ex : I come from bengaluru and I am 30 years old . (Output - entities={'location':'bengaluru','age':30})\n",
    "def get_ent(query,donor=False,recepient=False):\n",
    "    b_grp = ['a+','a-','b+','b-','ab+','ab-','o+','o-']\n",
    "    w = [i.lower() for i in word_tokenize(query)]\n",
    "    blood_group = [i for i in w if i in b_grp]\n",
    "    #print(blood_group)\n",
    "    if(len(blood_group)>0):\n",
    "        query = query.lower()\n",
    "        query = query.split(' ')\n",
    "        for i in query:\n",
    "            if blood_group[0] in i:\n",
    "                query.remove(i)\n",
    "\n",
    "        query = ' '.join(query)\n",
    "        blood_group = blood_group[0]\n",
    "    #print(query)\n",
    "    else:\n",
    "        blood_group = None\n",
    "    named_ents = processLanguage(query)\n",
    "    #print(named_ents)\n",
    "    if(len(named_ents)>0):\n",
    "        location = named_ents[0]\n",
    "    else:\n",
    "        location = None\n",
    "    units = re.findall(r'\\d+', query)\n",
    "    units = [int(i) for i in units]\n",
    "    if(len(units)>1):\n",
    "        unit = min(units)\n",
    "        age = max(units)\n",
    "    elif(len(units)==1):\n",
    "        if(units[0]<10):\n",
    "            unit = units[0]\n",
    "            age = None\n",
    "        else:\n",
    "            unit = None\n",
    "            age = units[0]\n",
    "    else:\n",
    "        unit = None\n",
    "        age = None\n",
    "    #if(units!=None):\n",
    "    #    units = units.group()\n",
    "    #print(units)\n",
    "    \n",
    "    if(donor==True):\n",
    "        ent_dict = {'age':age,'blood_group':blood_group,'location':location}\n",
    "        return ent_dict\n",
    "    elif(recepient==True):\n",
    "        ent_dict = {'age':age,'blood_group':blood_group,'location':location,'units_required':unit}\n",
    "        return ent_dict    \n",
    "\n",
    "    \n",
    "#default reply incase context is not set\n",
    "def context_not_set_handler(query):\n",
    "    return \"Will have to set context before asking this ...\"\n",
    "\n",
    "#function to handle greeting intent\n",
    "def greeting_handler(query):\n",
    "    l = [\"Hello, thanks for visiting\", \"Good to see you again\", \"Hi there, how can I help?\"]\n",
    "    return random.choice(l)\n",
    "    \n",
    "#function to handle goodbye intent\n",
    "def goodbye_handler(query):\n",
    "    l = [\"See you later, thanks for visiting\", \"Have a nice day\", \"Bye! Come back again soon.\"]\n",
    "    return random.choice(l)\n",
    "\n",
    "#function to handle thanking intent\n",
    "def thanks_handler(query):\n",
    "    l = [\"Happy to help!\", \"Any time!\", \"My pleasure\"]\n",
    "    return random.choice(l)\n",
    "\n",
    "#function to address query regarding time.\n",
    "def time_handler(query):\n",
    "    l = [\"The camps in the hospitals open at 9AM and close by 11PM .\"]\n",
    "    return random.choice(l)\n",
    "\n",
    "\n",
    "#function to address the 'willing to donate' intent\n",
    "#includes form action to obtain user details .\n",
    "#identifies entities if already specified using get_ent function.\n",
    "def want2donate_handler(query):\n",
    "    l = [\"Glad to hear that.\",\"Good to know.\",\"Pleased to hear that\"]\n",
    "    print(\"bot reply - \"+random.choice(l)+\"\\nI might need a few more details\")\n",
    "    questions = dict()\n",
    "    donor_details = dict()\n",
    "    questions['age'] = ['How old are you ?','May I know your age ?','What is your age ?']\n",
    "    questions['blood_group'] = ['What is your blood group ?',\"May I know your blood group ?\"]\n",
    "    questions['location'] = ['Name the city/town you live in .','Which city do you stay ?',\"Where do you live ?\",\"Please tell me your place of stay.\"]\n",
    "    \n",
    "    donor_details['age'] =[]\n",
    "    donor_details['blood_group'] = []\n",
    "    donor_details['location'] =[]\n",
    "    \n",
    "    ent_dict = get_ent(query,donor=True)\n",
    "    for i in ent_dict.keys():\n",
    "        if(ent_dict[i]!=None):\n",
    "            donor_details[i].append(ent_dict[i])\n",
    "    \n",
    "    b_grp = ['a+','a-','b+','b-','ab+','ab-','o+','o-']\n",
    "    \n",
    "    while(len(donor_details['age'])==0 or len(donor_details['blood_group'])==0 or len(donor_details['location'])==0):\n",
    "        a = random.choice(list(donor_details.keys()))\n",
    "        if(len(donor_details[a])==0):\n",
    "            print(random.choice(questions[a]))\n",
    "            if(a=='age'):\n",
    "                try:\n",
    "                    donor_details['age'].append(int(input()))\n",
    "                except:\n",
    "                    print(\"Please enter valid age\")\n",
    "                    while(True):\n",
    "                        try:\n",
    "                            donor_details['age'].append(int(input()))\n",
    "                            break\n",
    "                        except:\n",
    "                            print(\"Please enter valid age\")\n",
    "                            \n",
    "            elif(a=='blood_group'):\n",
    "                while(True):\n",
    "                    w = [i.lower() for i in word_tokenize(input())]\n",
    "                    bl_grp = [i for i in w if i in b_grp]\n",
    "                    if(len(bl_grp)>0):\n",
    "                        donor_details['blood_group'].append(bl_grp[0])\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Please enter valid blood group\")\n",
    "            elif(a=='location'):\n",
    "                while(True):\n",
    "                    w = processLanguage(input())\n",
    "                    if(len(w)>0):\n",
    "                        donor_details['location'].append(w[0])\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Please enter a valid location\")\n",
    "                    \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    print(\"Here are your details:\")\n",
    "    for i in list(donor_details.keys()):\n",
    "        print( i ,\":\", donor_details[i][0])\n",
    "    \n",
    "    if(donor_details['age'][0]<=50):\n",
    "        print(\"We would be drawing a maximum of 2 units of blood (350ml) from you .\")\n",
    "    else:\n",
    "        print(\"We would be drawing a unit of blood (350 ml) from you .\")\n",
    "    districts = [\"Kurnool\" , \"Guntur\" , \"Kadapa\" , \"Chitoor\",\"Anantapur\"]\n",
    "    dis = random.choice(districts)\n",
    "    print(dis+\" should be the closest to your place.\")\n",
    "    \n",
    "    print(\"Please visit any government hospital in \"+dis+\" district to donate blood\")\n",
    "    print(\"We appreciate your willingness . Thank you\")\n",
    "    return ''\n",
    "\n",
    "\n",
    "#function to address the 'willing to receive' intent\n",
    "#includes form action to obtain user details .\n",
    "#identifies entities if already specified using get_ent function.\n",
    "def want2receive_handler(query):\n",
    "    l = [\"Okay\",\"I see\",\"Sure\",\"Oh okay\",\"Oh\"]\n",
    "    print(\"bot reply - \"+random.choice(l)+\"\\nI might need a few more details\")\n",
    "    questions = dict()\n",
    "    recepient_details = dict()\n",
    "    questions['age'] = ['How old are you ?','May I know your age ?','What is your age ?']\n",
    "    questions['blood_group'] = ['What is your blood group ?',\"May I know your blood group ?\"]\n",
    "    questions['location'] = ['Name the city/town you live in .','Which city do you stay ?',\"Where do you live ?\",\"Please tell me your place of stay.\"]\n",
    "    questions['units_required'] = ['How many units of blood do you want ?','How many units of blood are required ?','How many units do you want ?',\"What's the quantity in need (in terms of units)?\"]\n",
    "    \n",
    "    recepient_details['age'] =[]\n",
    "    recepient_details['blood_group'] = []\n",
    "    recepient_details['location'] =[]\n",
    "    recepient_details['units_required'] = []\n",
    "    \n",
    "    ent_dict = get_ent(query,recepient=True)\n",
    "    for i in ent_dict.keys():\n",
    "        if(ent_dict[i]!=None):\n",
    "            recepient_details[i].append(ent_dict[i])\n",
    "    \n",
    "    b_grp = ['a+','a-','b+','b-','ab+','ab-','o+','o-']\n",
    "    \n",
    "    while(len(recepient_details['age'])==0 or len(recepient_details['blood_group'])==0 or len(recepient_details['location'])==0 or len(recepient_details['units_required'])==0):\n",
    "        a = random.choice(list(recepient_details.keys()))\n",
    "        if(len(recepient_details[a])==0):\n",
    "            print(random.choice(questions[a]))\n",
    "            if(a=='age'):\n",
    "                try:\n",
    "                    recepient_details['age'].append(int(input()))\n",
    "                except:\n",
    "                    print(\"Please enter valid age\")\n",
    "                    while(True):\n",
    "                        try:\n",
    "                            recepient_details['age'].append(int(input()))\n",
    "                            break\n",
    "                        except:\n",
    "                            print(\"Please enter valid age\")\n",
    "            elif(a=='blood_group'):\n",
    "                while(True):\n",
    "                    w = [i.lower() for i in word_tokenize(input())]\n",
    "                    bl_grp = [i for i in w if i in b_grp]\n",
    "                    if(len(bl_grp)>0):\n",
    "                        recepient_details['blood_group'].append(bl_grp[0])\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Please enter valid blood group\")\n",
    "                    \n",
    "            elif(a=='location'):\n",
    "                while(True):\n",
    "                    w = processLanguage(input())\n",
    "                    if(len(w)>0):\n",
    "                        recepient_details['location'].append(w[0])\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Please enter a valid location\")\n",
    "                    \n",
    "            elif(a=='units_required'):\n",
    "                try:\n",
    "                    recepient_details['units_required'].append(int(input()))\n",
    "                except:\n",
    "                    print(\"Please enter a number\")\n",
    "                    while(True):\n",
    "                        try:\n",
    "                            recepient_details['units_required'].append(int(input()))\n",
    "                            break\n",
    "                        except:\n",
    "                            print(\"Please enter a number\")\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    print(\"Here are your details:\")\n",
    "    for i in list(recepient_details.keys()):\n",
    "        print( i ,\":\", recepient_details[i][0])\n",
    "    \n",
    "    \n",
    "    districts = [\"Kurnool\" , \"Guntur\" , \"Kadapa\" , \"Chitoor\",\"Anantapur\"]\n",
    "    #dis = random.choice(districts)\n",
    "    #print(dis+\" should be the closest to your place.\")\n",
    "    \n",
    "    for dis in districts:\n",
    "        command = \"select dis_id from district where dname='\"+dis.lower()+\"';\"\n",
    "        did = list(conn.execute(command))[0][0][1]\n",
    "        conn.commit()\n",
    "        \n",
    "        command = \"select hname,hb_qty,h_id from hospital where dis_id = '\"+did+\"' and hb_grp='\"+recepient_details['blood_group'][0].upper()+\"';\"\n",
    "        b_qty = list(conn.execute(command))[0][1]\n",
    "        hname =list(conn.execute(command))[0][0]\n",
    "        h_id =list(conn.execute(command))[0][2]\n",
    "        if(int(b_qty)>=int(recepient_details['units_required'][0])):\n",
    "            print(\"Requirement found at hospital \",hname,\"in\",dis,\"district \")\n",
    "            print(\"You might be charged \",100*recepient_details['units_required'][0])\n",
    "            print(\"Please visit the hospital \")\n",
    "            print(\"Hope we have helped . Thank you\")\n",
    "            \n",
    "            #command = \"update hospital set hb_qty=\"+str(b_qty-int(recepient_details['units_required'][0]))+\" where h_id ='\"+str(h_id)+\"' and hb_grp ='\"+recepient_details['blood_group'][0].upper()+\"';\"                           \n",
    "            \n",
    "            \n",
    "            return ''\n",
    "    \n",
    "    print(\"Couldn't find requirement . Extremely sorry\")\n",
    "    return ''\n",
    "\n",
    "\n",
    "#Deals with conversation continuing statements like - hmm , okay , I see etc.\n",
    "def no_reply_handler(query):\n",
    "    return '...'\n",
    "\n",
    "\n",
    "\n",
    "def chat():\n",
    "    user_chat_history = []\n",
    "    bot_chat_history = []\n",
    "    print(\"Hello there !\")\n",
    "    query = \"\"\n",
    "    while(classify(query)[0][0]!='goodbye'):\n",
    "        query = input()\n",
    "        user_chat_history.append(query)\n",
    "        #s = response(query,show_details=True)\n",
    "        s = response(query)\n",
    "        #print(s)\n",
    "        reply = globals()[s](query)\n",
    "        '''\n",
    "        if(s in tables):\n",
    "            #reply = db_action(query,s)\n",
    "            reply = globals()[s+\"_handler\"](query,s)\n",
    "        else:\n",
    "            reply = s\n",
    "        '''\n",
    "        bot_chat_history.append(reply)\n",
    "        print(\"bot reply - \",reply)\n",
    "        print()\n",
    "    #capture_context(user_chat_history)\n",
    "    #return (user_chat_history,bot_chat_history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = {}\n",
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
